version: '3.8'

services:
  keycloak_db:
    image: postgres:14
    environment:
      POSTGRES_DB: keycloak_db
      POSTGRES_USER: keycloak_user
      POSTGRES_PASSWORD: keycloak_password
    volumes:
      - ./postgres-keycloak-data:/var/lib/postgresql/data
    ports:
      - "5433:5432"
    networks:
      - bionicpro-network

  keycloak:
    image: quay.io/keycloak/keycloak:21.1
    environment:
      KEYCLOAK_ADMIN: admin
      KEYCLOAK_ADMIN_PASSWORD: admin
      KC_DB: postgres
      KC_DB_URL: jdbc:postgresql://keycloak_db:5432/keycloak_db
      KC_DB_USERNAME: keycloak_user
      KC_DB_PASSWORD: keycloak_password
    command: 
      - start-dev
      - --import-realm
    volumes:
      - ./keycloak/realm-export.json:/opt/keycloak/data/import/realm-export.json
    ports:
      - "8080:8080"
    depends_on:
      - keycloak_db
    networks:
      - bionicpro-network

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - ./redis-data:/data
    command: redis-server --appendonly yes
    networks:
      - bionicpro-network

  openldap:
    image: osixia/openldap:1.5.0
    environment:
      LDAP_ORGANISATION: "BionicPRO"
      LDAP_DOMAIN: "example.com"
      LDAP_ADMIN_PASSWORD: "admin"
      LDAP_CONFIG_PASSWORD: "config"
      LDAP_READONLY_USER: "false"
      LDAP_RFC2307BIS_SCHEMA: "false"
      LDAP_BACKEND: "mdb"
      LDAP_TLS: "false"
      LDAP_REPLICATION: "false"
    volumes:
      - ./ldap/config.ldif:/container/service/slapd/assets/config/bootstrap/ldif/custom/50-bootstrap.ldif
      - ./ldap-data:/var/lib/ldap
      - ./ldap-config-data:/etc/ldap/slapd.d
    ports:
      - "389:389"
      - "636:636"
    networks:
      - bionicpro-network

  bionicpro-auth:
    build:
      context: ./bionicpro-auth
      dockerfile: Dockerfile
    ports:
      - "8081:8081"
    environment:
      KEYCLOAK_URL: http://keycloak:8080
      KEYCLOAK_REALM: reports-realm
      KEYCLOAK_CLIENT_ID: bionicpro-auth
      KEYCLOAK_CLIENT_SECRET: bionicpro-auth-secret-change-in-production
      REDIS_HOST: redis
      REDIS_PORT: 6379
      ENCRYPTION_KEY: ${ENCRYPTION_KEY:-default-encryption-key-change-in-production}
    depends_on:
      - keycloak
      - redis
    networks:
      - bionicpro-network

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:3000"
    environment:
      REACT_APP_API_URL: http://localhost:8000
      REACT_APP_AUTH_SERVICE_URL: http://localhost:8081
      REACT_APP_KEYCLOAK_URL: http://localhost:8080
      REACT_APP_KEYCLOAK_REALM: reports-realm
      REACT_APP_KEYCLOAK_CLIENT_ID: reports-frontend
    depends_on:
      - bionicpro-auth
    networks:
      - bionicpro-network

  # CRM Database (PostgreSQL)
  crm_db:
    image: postgres:14
    environment:
      POSTGRES_DB: crm_db
      POSTGRES_USER: crm_user
      POSTGRES_PASSWORD: crm_password
    volumes:
      - ./Task2/db-init/crm-init.sql:/docker-entrypoint-initdb.d/init.sql
      - ./crm-data:/var/lib/postgresql/data
    ports:
      - "5434:5432"
    command: postgres -c wal_level=logical -c max_replication_slots=4 -c max_wal_senders=4
    networks:
      - bionicpro-network

  # Telemetry Database (PostgreSQL)
  telemetry_db:
    image: postgres:14
    environment:
      POSTGRES_DB: telemetry_db
      POSTGRES_USER: telemetry_user
      POSTGRES_PASSWORD: telemetry_password
    volumes:
      - ./Task2/db-init/telemetry-init.sql:/docker-entrypoint-initdb.d/init.sql
      - ./telemetry-data:/var/lib/postgresql/data
    ports:
      - "5435:5432"
    networks:
      - bionicpro-network

  # ClickHouse OLAP Database
  clickhouse:
    image: clickhouse/clickhouse-server:latest
    environment:
      CLICKHOUSE_DB: reports_db
      CLICKHOUSE_USER: clickhouse_user
      CLICKHOUSE_PASSWORD: clickhouse_password
    volumes:
      - ./clickhouse-data:/var/lib/clickhouse
      - ./Task2/db-init/clickhouse-init.sql:/docker-entrypoint-initdb.d/01-init.sql
      - ./Task4/db-init/clickhouse-cdc-init.sql:/docker-entrypoint-initdb.d/02-cdc-init.sql
    ports:
      - "8123:8123"
      - "9000:9000"
    depends_on:
      kafka:
        condition: service_started
    networks:
      - bionicpro-network

  # Airflow Postgres (для метаданных Airflow)
  airflow_db:
    image: postgres:14
    environment:
      POSTGRES_DB: airflow
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
    volumes:
      - ./airflow-db-data:/var/lib/postgresql/data
    networks:
      - bionicpro-network

  # Airflow Redis (для CeleryExecutor, если используется)
  airflow_redis:
    image: redis:7-alpine
    networks:
      - bionicpro-network

  # Airflow Webserver
  airflow_webserver:
    image: apache/airflow:2.8.0
    command: webserver
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow_db/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'
      _AIRFLOW_DB_UPGRADE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: airflow
      _AIRFLOW_WWW_USER_PASSWORD: airflow
    volumes:
      - ./Task2/airflow/dags:/opt/airflow/dags
      - ./Task2/airflow/logs:/opt/airflow/logs
      - ./Task2/airflow/plugins:/opt/airflow/plugins
      - ./Task2/airflow/config:/opt/airflow/config
    ports:
      - "8082:8080"
    depends_on:
      - airflow_db
    networks:
      - bionicpro-network

  # Airflow Scheduler
  airflow_scheduler:
    image: apache/airflow:2.8.0
    command: scheduler
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow_db/airflow
      AIRFLOW__CORE__FERNET_KEY: ''
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'
    volumes:
      - ./Task2/airflow/dags:/opt/airflow/dags
      - ./Task2/airflow/logs:/opt/airflow/logs
      - ./Task2/airflow/plugins:/opt/airflow/plugins
      - ./Task2/airflow/config:/opt/airflow/config
    depends_on:
      - airflow_db
      - airflow_webserver
    networks:
      - bionicpro-network

  # Minio S3 Storage
  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    ports:
      - "9002:9000"
      - "9001:9001"
    volumes:
      - ./minio-data:/data
    networks:
      - bionicpro-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3

  # Nginx CDN
  nginx-cdn:
    image: nginx:alpine
    ports:
      - "8083:80"
    volumes:
      - ./Task3/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./Task3/nginx/cache:/var/cache/nginx
    depends_on:
      - minio
    networks:
      - bionicpro-network

  # Reports Service
  bionicpro-reports:
    build:
      context: ./bionicpro-reports
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      CLICKHOUSE_HOST: clickhouse
      CLICKHOUSE_PORT: 9000
      CLICKHOUSE_DB: reports_db
      CLICKHOUSE_USER: clickhouse_user
      CLICKHOUSE_PASSWORD: clickhouse_password
      AUTH_SERVICE_URL: http://bionicpro-auth:8081
      KEYCLOAK_URL: http://keycloak:8080
      KEYCLOAK_REALM: reports-realm
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: minioadmin
      S3_SECRET_KEY: minioadmin
      S3_BUCKET: reports
      CDN_BASE_URL: http://nginx-cdn
      CDN_PUBLIC_URL: http://localhost:8083
    depends_on:
      - clickhouse
      - bionicpro-auth
      - minio
    networks:
      - bionicpro-network

  # Zookeeper для Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - bionicpro-network

  # Kafka
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "9093:9093"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    networks:
      - bionicpro-network

  # Debezium Connect
  debezium-connect:
    image: debezium/connect:2.4
    ports:
      - "8084:8083"
    depends_on:
      - kafka
      - crm_db
    environment:
      BOOTSTRAP_SERVERS: kafka:29092
      GROUP_ID: 1
      CONFIG_STORAGE_TOPIC: my_connect_configs
      OFFSET_STORAGE_TOPIC: my_connect_offsets
      STATUS_STORAGE_TOPIC: my_connect_statuses
      CONFIG_STORAGE_REPLICATION_FACTOR: 1
      OFFSET_STORAGE_REPLICATION_FACTOR: 1
      STATUS_STORAGE_REPLICATION_FACTOR: 1
    volumes:
      - ./Task4/debezium/connectors:/kafka/connect
    networks:
      - bionicpro-network

networks:
  bionicpro-network:
    driver: bridge
