# Чеклист для проверки перед отправкой задания 4

## Общие требования

- [ ] Все файлы сохранены в папке `Task4/`
- [ ] Документация создана и актуальна
- [ ] Код соответствует стандартам проекта

## 1. Настройка инфраструктуры

### Docker Compose
- [ ] Добавлены сервисы Kafka и Zookeeper в `docker-compose.yaml`
- [ ] Добавлен сервис Debezium Connect в `docker-compose.yaml`
- [ ] PostgreSQL CRM настроен с параметрами `wal_level=logical`
- [ ] ClickHouse настроен с зависимостью от Kafka
- [ ] Все сервисы находятся в одной сети `bionicpro-network`

### Проверка запуска сервисов
- [ ] Все сервисы успешно запускаются: `docker-compose up -d`
- [ ] Kafka доступен на порту 9092
- [ ] Zookeeper доступен на порту 2181
- [ ] Debezium Connect доступен на порту 8084
- [ ] PostgreSQL CRM доступен на порту 5434
- [ ] ClickHouse доступен на портах 8123 и 9000

## 2. Debezium Connector

### Конфигурация
- [ ] Создан файл `Task4/debezium/connectors/crm-connector.json`
- [ ] Конфигурация содержит правильные параметры подключения к PostgreSQL
- [ ] Указаны таблицы для отслеживания: `public.users`, `public.prostheses`
- [ ] Настроен плагин `pgoutput`
- [ ] Настроены трансформации для извлечения состояния записи

### Регистрация и проверка
- [ ] Connector успешно зарегистрирован через REST API
- [ ] Статус connector'а `RUNNING`
- [ ] В логах Debezium нет ошибок
- [ ] Созданы топики в Kafka: `crm_db.public.users` и `crm_db.public.prostheses`

### Тестирование CDC
- [ ] Вставка данных в таблицу `users` реплицируется в Kafka
- [ ] Вставка данных в таблицу `prostheses` реплицируется в Kafka
- [ ] Обновление данных реплицируется корректно
- [ ] Удаление данных обрабатывается корректно

## 3. ClickHouse KafkaEngine

### Таблицы KafkaEngine
- [ ] Создана таблица `users_kafka` с движком Kafka
- [ ] Создана таблица `prostheses_kafka` с движком Kafka
- [ ] Настроены правильные параметры подключения к Kafka
- [ ] Указаны правильные топики для чтения

### Целевые таблицы
- [ ] Создана таблица `users_target` с движком ReplacingMergeTree
- [ ] Создана таблица `prostheses_target` с движком ReplacingMergeTree
- [ ] Правильно настроен ключ сортировки для дедупликации

### MaterializedView
- [ ] Создан MaterializedView `users_mv` для загрузки данных из `users_kafka`
- [ ] Создан MaterializedView `prostheses_mv` для загрузки данных из `prostheses_kafka`
- [ ] Данные автоматически загружаются из Kafka в целевые таблицы

### Проверка данных
- [ ] Данные из Kafka появляются в `users_target`
- [ ] Данные из Kafka появляются в `prostheses_target`
- [ ] Использование `FINAL` в запросах возвращает актуальные данные
- [ ] Дедупликация работает корректно

## 4. Витрина данных

### CRM Data Mart
- [ ] Создана таблица `crm_data_mart` для витрины данных
- [ ] Создан MaterializedView `crm_data_mart_mv` для автоматического обновления
- [ ] Витрина объединяет данные из `users_target` и `prostheses_target`
- [ ] Данные в витрине актуальны и корректны

### Проверка витрины
- [ ] Запрос к `crm_data_mart FINAL` возвращает данные
- [ ] Данные соответствуют данным в исходных таблицах
- [ ] Обновления в исходных таблицах отражаются в витрине

## 5. Обновленная витрина для отчётов

### Prosthesis Reports Mart
- [ ] Создана таблица `prosthesis_reports_mart`
- [ ] Таблица партиционирована по месяцам
- [ ] Правильно настроен ключ сортировки
- [ ] Создан скрипт для объединения данных телеметрии

### Проверка витрины
- [ ] Данные из CRM и телеметрии объединяются корректно
- [ ] Запросы к витрине выполняются быстро
- [ ] Данные актуальны

## 6. Обновление API сервиса

### ReportService
- [ ] Метод `getReportsForUser()` обновлен для использования `prosthesis_reports_mart`
- [ ] Метод `isDataProcessedForPeriod()` обновлен для использования `prosthesis_reports_mart`
- [ ] Запросы используют правильное имя таблицы
- [ ] Код компилируется без ошибок

### Тестирование API
- [ ] API возвращает данные из новой витрины
- [ ] Запросы выполняются быстрее, чем раньше
- [ ] Нет ошибок в логах сервиса

## 7. Документация

### IMPLEMENTATION.md
- [ ] Описана архитектура решения
- [ ] Описан поток данных
- [ ] Приведены инструкции по настройке
- [ ] Описаны команды для проверки работы
- [ ] Указаны преимущества решения
- [ ] Описаны способы устранения неполадок

### CHECKLIST.md
- [ ] Чеклист содержит все необходимые пункты проверки
- [ ] Пункты сгруппированы по логическим разделам
- [ ] Указаны конкретные команды для проверки

## 8. Функциональное тестирование

### Полный цикл работы
- [ ] Вставка данных в PostgreSQL CRM → появление в Kafka → появление в ClickHouse
- [ ] Обновление данных в PostgreSQL CRM → обновление в ClickHouse
- [ ] API возвращает актуальные данные из витрины
- [ ] Нет влияния на производительность OLTP операций в CRM

### Производительность
- [ ] Запросы к API выполняются быстро
- [ ] Нет блокировок в PostgreSQL CRM при выгрузке данных
- [ ] ClickHouse обрабатывает данные в реальном времени

## 9. Проверка перед отправкой

### Финальная проверка
- [ ] Все сервисы запускаются без ошибок
- [ ] CDC работает корректно
- [ ] Данные реплицируются в реальном времени
- [ ] API использует новую витрину
- [ ] Документация полная и актуальная
- [ ] Все файлы находятся в папке `Task4/`

### Команды для финальной проверки

```bash
# 1. Проверка статуса всех сервисов
docker-compose ps

# 2. Проверка статуса Debezium connector
curl http://localhost:8084/connectors/crm-postgres-connector/status

# 3. Проверка топиков Kafka
docker exec -it <kafka-container> kafka-topics --list --bootstrap-server localhost:9092

# 4. Проверка данных в ClickHouse
docker exec -it <clickhouse-container> clickhouse-client -q "SELECT COUNT(*) FROM reports_db.users_target"

# 5. Проверка витрины
docker exec -it <clickhouse-container> clickhouse-client -q "SELECT COUNT(*) FROM reports_db.crm_data_mart FINAL"

# 6. Тест API
curl -H "Authorization: Bearer <token>" http://localhost:8000/api/reports
```

## 10. Дополнительные проверки

### Безопасность
- [ ] Пароли не захардкожены в конфигурации
- [ ] Используются переменные окружения для чувствительных данных
- [ ] Сетевая изоляция настроена корректно

### Логирование
- [ ] Логи Debezium не содержат ошибок
- [ ] Логи Kafka не содержат ошибок
- [ ] Логи ClickHouse не содержат ошибок
- [ ] Логи API сервиса не содержат ошибок

### Резервное копирование
- [ ] Настроено резервное копирование данных ClickHouse (если требуется)
- [ ] Настроено резервное копирование конфигурации Kafka (если требуется)

---

**Примечание**: Отметьте все пункты перед отправкой задания. Если какой-то пункт не выполнен, укажите причину в комментариях.

